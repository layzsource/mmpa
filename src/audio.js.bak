// ðŸŽ¶ Audio Engine â€” Phase 13.1 (Electron-safe, single-init, no prompts)
// - No automatic mic popups; initializes on first start() or user gesture.
// - Survives Electron focus/visibility changes.
// - Safe if no input device; logs but never crashes.
// - Provides analyser spectrum + RMS; optional self-register into hudCallbacks.

console.log("ðŸ§  AUDIO DEBUG PROBE ACTIVE â€” Phase 13.1a");

import { AudioBandFilter } from './audioKalmanFilter.js';
import { AudioMMPABridge } from './audioMMPABridge.js';
import { DropPredictor } from './audioAnalysis.js';
import { FullSpectrumDropPredictor } from './audioAnalysisFull.js';

const LOG = (...args) => console.log("audio.js:", ...args);
LOG("ðŸŽ¶ audio.js loaded (Phase 13.1)");

// Phase 13.13 â€” Global Reactivity Gain (0.0 .. 4.0), default 1.0
function _scaleBands(b) {
  const g = Math.max(0, Math.min(4.0, (window.ReactivityGain ??= 1.0)));
  if (g === 1) return b;
  return {
    bass:   Math.min(1, (b?.bass   ?? 0) * g),
    mid:    Math.min(1, (b?.mid    ?? 0) * g),
    treble: Math.min(1, (b?.treble ?? 0) * g),
    level:  Math.min(1, (b?.level  ?? 0) * g),
  };
}

class AudioCore {
  constructor() {
    this.ctx = null;
    this.source = null;         // MediaStreamSource
    this.analyser = null;
    this.inputGain = null;      // GainNode before analyser (metering/headroom)
    this.stream = null;
    this.deviceId = null;
    this.state = "idle";        // idle | ready | running | error
    this.fftSize = 2048;
    this.smoothing = 0.8;
    this.freqData = null;
    this.timeData = null;
    this.onReadyCbs = new Set();
    this.onErrorCbs = new Set();

    // Phase 13.4: Event emitter for frame updates
    this.frameListeners = new Set();
    this.bands = { bass: 0, mid: 0, treble: 0, level: 0 };
    this.frameLoop = null;

    // Phase 13.29: Kalman-LQR filtering for audio bands
    this.kalmanFilter = new AudioBandFilter({
      preset: 'balanced',  // smooth, balanced, responsive, reactive
      enabled: true
    });
    this.rawBands = { bass: 0, mid: 0, treble: 0, level: 0 }; // Store raw for comparison

    // Phase 13.30: MMPA V2.0 Audio Bridge (predictive bifurcation detection)
    this.mmpaBridge = new AudioMMPABridge({
      enabled: true,  // ENABLED: Using MMPA V2.0 predictions (UKF/LQR/FIM)
      audioFeatureMode: 'bands'  // Use band-based features
    });
    this.mmpaData = null; // Store MMPA predictions/attribution
    this.mmpaLogCounter = 0; // Counter for periodic logging
    console.log("ðŸŽ¯ MMPA V2.0 Bridge initialized:", {
      enabled: this.mmpaBridge.enabled,
      mode: this.mmpaBridge.audioFeatureMode
    });

    // Phase 13.31: Real Audio Analysis (onset detection, beat tracking, drop prediction)
    // Will be initialized after audio context is created
    this.dropPredictor = null;
    this.audioAnalysisData = null; // Store real audio analysis results
    this.analysisLogCounter = 0;

    // one-time handlers
    this._boundResume = this._resumeIfSuspended.bind(this);
    this._visibility = this._handleVisibility.bind(this);
    document.addEventListener("visibilitychange", this._visibility);
    window.addEventListener("focus", this._boundResume);
  }

  // ðŸ§© Phase 13.8b â€” Test Tone feeds ANALYSER chain (not speakers)
  async toggleTestTone(enable = !this.testToneActive) {
    await this._ensureContext();
    if (enable && !this.testToneOsc) {
      console.log("ðŸ“¢ AudioEngine: Test Tone ON @ 220 Hz (to analyser)");
      // build osc -> toneGain -> inputGain -> analyser
      const osc = this.ctx.createOscillator();
      const toneGain = this.ctx.createGain();
      osc.type = "sine";
      osc.frequency.value = 220;
      toneGain.gain.value = 0.25;

      // disconnect mic source from inputGain while tone is active
      try { this.source?.disconnect(this.inputGain); } catch {}

      osc.connect(toneGain).connect(this.inputGain);
      osc.start();

      this.testToneOsc = osc;
      this.testToneGain = toneGain;
      this.testToneActive = true;
      return;
    }

    if (!enable && this.testToneOsc) {
      console.log("ðŸ”‡ AudioEngine: Test Tone OFF");
      try {
        this.testToneOsc.stop();
        this.testToneOsc.disconnect();
        this.testToneGain.disconnect();
      } catch {}
      this.testToneOsc = null;
      this.testToneGain = null;
      this.testToneActive = false;

      // restore mic chain if available
      if (this.source && this.inputGain) {
        try { this.source.connect(this.inputGain); } catch {}
      }
    }
  }

  // ðŸŽ¹ Connect external audio source (e.g., synth) to analyzer
  async connectExternalSource(sourceNode) {
    await this._ensureContext();

    // Disconnect mic and test tone if active
    try { this.source?.disconnect(this.inputGain); } catch {}
    if (this.testToneActive) {
      await this.toggleTestTone(false);
    }

    // Store reference to external source
    this.externalSource = sourceNode;

    // Connect external source to analyzer chain
    sourceNode.connect(this.inputGain);

    console.log("ðŸŽ¹ AudioEngine: External source connected to analyzer");
  }

  // Disconnect external source and restore mic input
  disconnectExternalSource() {
    if (this.externalSource) {
      try {
        this.externalSource.disconnect(this.inputGain);
      } catch {}
      this.externalSource = null;

      // Restore mic chain if available
      if (this.source && this.inputGain) {
        try { this.source.connect(this.inputGain); } catch {}
      }

      console.log("ðŸŽ¹ AudioEngine: External source disconnected, mic restored");
    }
  }

  // List audio input devices (mics, loopbacks)
  async listInputs() {
    if (!navigator.mediaDevices?.enumerateDevices) return [];
    const devices = await navigator.mediaDevices.enumerateDevices();
    return devices.filter(d => d.kind === "audioinput");
  }

  // public: switch device and rewire into analyser
  async selectDevice(deviceId) {
    this.deviceId = deviceId || null;
    await this._ensureInput(true);
    // if a test tone is active, it still goes into inputGain; both are okay
    console.log("ðŸŽ›ï¸ AudioEngine: switched input", this.deviceId || "(default)");
  }

  // small helper to bump/trim preamp from HUD
  setPreGain(mult) {
    if (!this.preGain) return;
    this.preGain.gain.value = Math.max(0.1, Math.min(16.0, mult));
    console.log("ðŸ”‰ AudioEngine preGain =", this.preGain.gain.value.toFixed(2), "x");
  }

  // Phase 13.29: Kalman-LQR filter control methods
  setKalmanPreset(preset) {
    // preset: 'smooth', 'balanced', 'responsive', 'reactive'
    if (!this.kalmanFilter) return;
    this.kalmanFilter.setPreset(preset);
    console.log(`ðŸŽµ AudioEngine: Kalman filter preset changed to '${preset}'`);
  }

  setKalmanEnabled(enabled) {
    if (!this.kalmanFilter) return;
    this.kalmanFilter.setEnabled(enabled);
    console.log(`ðŸŽµ AudioEngine: Kalman filter ${enabled ? 'enabled' : 'disabled (bypass)'}`);
  }

  getKalmanDiagnostics() {
    if (!this.kalmanFilter) return null;
    return this.kalmanFilter.getDiagnostics();
  }

  // Phase 13.30: MMPA V2.0 control methods
  setMMPAEnabled(enabled) {
    if (!this.mmpaBridge) return;
    this.mmpaBridge.enabled = enabled;
    console.log(`ðŸŽµ AudioEngine: MMPA bridge ${enabled ? 'enabled' : 'disabled'}`);
  }

  getMMPAData() {
    return this.mmpaData;
  }

  getMMPAHUDData() {
    if (!this.mmpaBridge) return null;
    return this.mmpaBridge.getHUDData();
  }

  getMMPADiagnostics() {
    if (!this.mmpaBridge) return null;
    return this.mmpaBridge.getDiagnostics();
  }

  resetMMPA() {
    if (!this.mmpaBridge) return;
    this.mmpaBridge.reset();
    this.mmpaData = null;
    console.log('ðŸŽµ AudioEngine: MMPA bridge reset');
  }

  // Phase 13.31: Audio Analysis control methods
  setAudioAnalysisEnabled(enabled) {
    if (!this.dropPredictor) return;
    this.dropPredictor.enabled = enabled;
    console.log(`ðŸŽµ AudioEngine: Audio analysis ${enabled ? 'enabled' : 'disabled'}`);
  }

  getAudioAnalysisData() {
    return this.audioAnalysisData;
  }

  getAudioAnalysisHUDData() {
    if (!this.dropPredictor) return null;
    return this.dropPredictor.getHUDData();
  }

  getAudioAnalysisDiagnostics() {
    if (!this.dropPredictor) return null;
    return this.dropPredictor.getDiagnostics();
  }

  resetAudioAnalysis() {
    if (!this.dropPredictor) return;
    this.dropPredictor.reset();
    this.audioAnalysisData = null;
    console.log('ðŸŽµ AudioEngine: Audio analysis reset');
  }

  tap() {
    if (!this.dropPredictor) return;
    this.dropPredictor.tap();
  }

  resetTapTempo() {
    if (!this.dropPredictor) return;
    this.dropPredictor.resetTapTempo();
  }

  // ---- public API ----------------------------------------------------------

  async init(opts = {}) {
    if (this.state !== "idle") return;
    this.fftSize = opts.fftSize || this.fftSize;
    this.smoothing = opts.smoothing ?? this.smoothing;
    this.deviceId = opts.deviceId || null;
    try {
      await this._ensureContext();
      await this._ensureInput();
      this._ensureAnalyser();
      this.state = "ready";
      LOG("âœ… Audio engine ready");
      this._notify(this.onReadyCbs);
    } catch (err) {
      this.state = "error";
      console.error("audio.js: âŒ init failed:", err);
      this._notify(this.onErrorCbs, err);
    }
  }

  async start() {
    console.log("ðŸ” Phase 13.1a: AudioEngine.start() called, state=", this.state);
    // lazy init
    if (this.state === "idle") await this.init();
    if (this.state === "error") return false;
    await this._resumeIfSuspended();
    this.state = "running";

    // Phase 13.4: Start frame loop for event broadcasting
    if (!this.frameLoop) {
      this._startFrameLoop();
    }

    console.log("ðŸ” Phase 13.1a: AudioEngine.start() complete, state=", this.state);
    return true;
  }

  // Phase 13.4: Event emitter methods
  on(event, callback) {
    if (event === 'frame' && typeof callback === 'function') {
      this.frameListeners.add(callback);
    }
  }

  off(event, callback) {
    if (event === 'frame') {
      this.frameListeners.delete(callback);
    }
  }

  emit(event, data) {
    if (event === 'frame') {
      for (const listener of this.frameListeners) {
        try {
          listener(data);
        } catch (e) {
          console.error("AudioEngine frame listener error:", e);
        }
      }
    }
  }

  _startFrameLoop() {
    let loopFrameCount = 0;
    const update = () => {
      if (this.state !== "running") {
        this.frameLoop = null;
        LOG("ðŸ›‘ Frame loop stopped, state:", this.state);
        return;
      }

      this.updateBands();
      // Phase 13.13: emit scaled bands
      this.emit('frame', _scaleBands(this.bands));

      // Log every 120 frames (every 2 seconds at 60fps)
      if (loopFrameCount++ % 120 === 0) {
        LOG(`ðŸŽ§ Frame loop tick #${loopFrameCount}, listeners: ${this.frameListeners.size}, bands:`, this.bands);
      }

      this.frameLoop = requestAnimationFrame(update);
    };
    this.frameLoop = requestAnimationFrame(update);
    LOG("ðŸŽ§ AudioEngine frame loop started, listeners:", this.frameListeners.size);
  }

  updateBands() {
    if (!this.analyser) return;

    const spectrum = this.getSpectrum();
    const rms = this.getRMS();
    const n = spectrum.length;

    if (n > 0) {
      const bassEnd = Math.floor(n * 0.15);
      const midEnd = Math.floor(n * 0.6);
      const avg = (arr, a, b) => arr.slice(a, b).reduce((s, v) => s + v, 0) / (b - a);

      // Compute raw bands from FFT
      this.rawBands = {
        bass: avg(spectrum, 0, bassEnd) / 255,
        mid: avg(spectrum, bassEnd, midEnd) / 255,
        treble: avg(spectrum, midEnd, n) / 255,
        level: rms
      };

      // Apply Kalman-LQR filtering for stable, smooth output
      this.bands = this.kalmanFilter.update(this.rawBands);

      // Phase 13.30: Run MMPA V2.0 prediction cycle
      if (this.mmpaBridge && this.mmpaBridge.enabled) {
        this.mmpaData = this.mmpaBridge.update(this.bands);

        // Log MMPA data every 300 frames (every 5 seconds at 60fps) for debugging
        this.mmpaLogCounter++;
        if (this.mmpaLogCounter % 300 === 0 && this.mmpaData) {
          console.log("ðŸŽ¯ MMPA Update:", {
            enabled: this.mmpaData.enabled,
            sigma_star: this.mmpaData.sigma_star?.toFixed(3),
            risk: (this.mmpaData.bifurcation_risk * 100)?.toFixed(1) + '%',
            dominant_band: this.mmpaData.dominant_band,
            warning: this.mmpaData.predictions?.transitionWarning
          });
        }
      }

      // Phase 13.31: Run Full-Spectrum Audio Analysis (Drop Prediction)
      if (this.dropPredictor) {
        this.audioAnalysisData = this.dropPredictor.update(); // No parameters - reads directly from analyser

        // The new Full-Spectrum predictor has built-in logging every 3 seconds
        // Removed redundant logging here
      }

      // after computing this.bands = { bass, mid, treble, level }
      if (!this._printedNonZero && (this.bands.level > 0.02)) {
        this._printedNonZero = true;
        console.log("âœ… Mic bands active:", this.bands);
      }
    }
  }

  async stop() {
    try {
      if (this.stream) {
        for (const t of this.stream.getTracks()) t.stop();
      }
    } catch {}
    this.stream = null;
    this.source = null;
    // keep ctx alive; we just drop input
    this.state = "ready";
  }

  async setDeviceId(deviceId) {
    if (deviceId === this.deviceId) return;
    this.deviceId = deviceId || null;
    if (!this.ctx) await this._ensureContext();
    await this._ensureInput(true); // force rebuild source
    this._ensureAnalyser();        // relink graph
    LOG("ðŸ” Switched audio device:", this.deviceId || "(default)");
  }

  onReady(cb) { this.onReadyCbs.add(cb); return () => this.onReadyCbs.delete(cb); }
  onError(cb) { this.onErrorCbs.add(cb); return () => this.onErrorCbs.delete(cb); }

  getAnalyser() { return this.analyser; }

  getRMS() {
    if (!this.analyser) return 0;
    if (!this.timeData) this.timeData = new Uint8Array(this.analyser.fftSize);
    this.analyser.getByteTimeDomainData(this.timeData);
    let sum = 0;
    for (let i = 0; i < this.timeData.length; i++) {
      const v = (this.timeData[i] - 128) / 128; // [-1,1]
      sum += v * v;
    }
    return Math.sqrt(sum / this.timeData.length);
  }

  getSpectrum() {
    if (!this.analyser) return new Uint8Array(0);
    if (!this.freqData) this.freqData = new Uint8Array(this.analyser.frequencyBinCount);
    this.analyser.getByteFrequencyData(this.freqData);
    return this.freqData;
  }

  // Optional: tick for HUD
  tick() {
    // no-op placeholder; HUD can poll getRMS/getSpectrum
  }

  // ---- internals -----------------------------------------------------------

  async _ensureContext() {
    if (this.ctx) return;
    const Ctx = window.AudioContext || window.webkitAudioContext;
    if (!Ctx) throw new Error("WebAudio not supported");
    this.ctx = new Ctx({ latencyHint: "interactive" });
    LOG("ðŸ§  AudioContext created:", this.ctx.state);
  }

  // âœ… Phase 13.9 â€” Robust mic path into analyser with pre-gain
  async _ensureInput(recreate = false) {
    await this._ensureContext();
    if (!this.inputGain) this.inputGain = this.ctx.createGain();
    if (!this.preGain)   this.preGain   = this.ctx.createGain(); // preamp for weak mics
    // default preamp ~ +12dB (â‰ˆ4x). We'll expose this in HUD too.
    if (typeof this.preGain.gain.value !== "number" || this.preGain.gain.value === 0) {
      this.preGain.gain.value = 4.0;
    }

    // (re)create stream
    if (recreate || !this.stream) {
      // close prior
      try { this.source?.disconnect(); } catch {}
      try { this.stream?.getTracks?.().forEach(t => t.stop()); } catch {}

      const constraints = this.deviceId
        ? { audio: { deviceId: { exact: this.deviceId } } }
        : { audio: true };

      this.stream = await navigator.mediaDevices.getUserMedia(constraints);
      this.source = this.ctx.createMediaStreamSource(this.stream);
    }

    // wiring: source â†’ preGain â†’ inputGain â†’ analyser
    this._ensureAnalyser();
    try { this.source.disconnect(); } catch {}
    try { this.preGain.disconnect(); } catch {}
    try { this.inputGain.disconnect(); } catch {}

    this.source.connect(this.preGain);
    this.preGain.connect(this.inputGain);
    this.inputGain.connect(this.analyser);

    // do NOT connect to destination here (analyser only)
    this.state = "ready";
    console.log("ðŸŽ™ï¸ Mic path wired â†’ analyser (preGain:", this.preGain.gain.value.toFixed(2) + ")");
  }

  _ensureAnalyser() {
    if (!this.ctx || !this.source) return;
    // (re)build nodes
    if (!this.inputGain) this.inputGain = this.ctx.createGain();
    this.inputGain.gain.value = 1.0;

    if (!this.analyser) this.analyser = this.ctx.createAnalyser();
    this.analyser.fftSize = this.fftSize;
    this.analyser.smoothingTimeConstant = this.smoothing;

    // Phase 13.31: Initialize Full-Spectrum Drop Predictor after analyser is ready
    if (!this.dropPredictor) {
      this.dropPredictor = new FullSpectrumDropPredictor(this.ctx, this.analyser, {
        onsetThreshold: 15  // Flux threshold for onset detection
      });
      console.log("ðŸŽµ FullSpectrumDropPredictor initialized with", this.analyser.frequencyBinCount, "FFT bins");
    }

    // wire: source -> inputGain -> analyser (no audio to destination)
    try {
      this.source.disconnect();
    } catch {}
    this.inputGain.disconnect?.();
    this.analyser.disconnect?.();

    this.source.connect(this.inputGain);
    this.inputGain.connect(this.analyser);
  }

  async _resumeIfSuspended() {
    if (!this.ctx) return;
    if (this.ctx.state === "suspended") {
      try {
        await this.ctx.resume();
        LOG("â–¶ï¸ AudioContext resumed");
      } catch (e) {
        console.warn("AudioContext resume failed:", e);
      }
    }
  }

  _handleVisibility() {
    if (document.visibilityState === "visible") this._resumeIfSuspended();
  }

  _notify(set, arg) { for (const cb of set) try { cb(arg); } catch (e) { console.error(e); } }
}

// Singleton instance
export const AudioEngine = new AudioCore();

// Expose globally for DevTools testing
if (typeof window !== "undefined") {
  window.AudioEngine = AudioEngine;
  window.AudioProbe = {
    // Phase 13.4: Enhanced probe with event testing
    start: () => AudioEngine.start(),
    stop: () => AudioEngine.stop(),
    info: () => ({
      ctx: AudioEngine.ctx?.state,
      hasAnalyser: !!AudioEngine.analyser,
      state: AudioEngine.state,
      frameListeners: AudioEngine.frameListeners.size,
      bands: AudioEngine.bands,
    }),
    getBands: () => AudioEngine.bands,
    getRawBands: () => AudioEngine.rawBands,
    getRMS: () => AudioEngine.getRMS(),
    getSpectrum: () => AudioEngine.getSpectrum(),
    // Phase 13.29: Kalman-LQR filter controls
    setKalmanPreset: (preset) => AudioEngine.setKalmanPreset(preset),
    setKalmanEnabled: (enabled) => AudioEngine.setKalmanEnabled(enabled),
    getKalmanDiagnostics: () => AudioEngine.getKalmanDiagnostics(),
    // Phase 13.30: MMPA V2.0 controls
    setMMPAEnabled: (enabled) => AudioEngine.setMMPAEnabled(enabled),
    getMMPAData: () => AudioEngine.getMMPAData(),
    getMMPAHUDData: () => AudioEngine.getMMPAHUDData(),
    getMMPADiagnostics: () => AudioEngine.getMMPADiagnostics(),
    resetMMPA: () => AudioEngine.resetMMPA(),
    // Phase 13.31: Audio Analysis controls (onset detection, beat tracking, drop prediction)
    setAudioAnalysisEnabled: (enabled) => AudioEngine.setAudioAnalysisEnabled(enabled),
    getAudioAnalysisData: () => AudioEngine.getAudioAnalysisData(),
    getAudioAnalysisHUDData: () => AudioEngine.getAudioAnalysisHUDData(),
    getAudioAnalysisDiagnostics: () => AudioEngine.getAudioAnalysisDiagnostics(),
    resetAudioAnalysis: () => AudioEngine.resetAudioAnalysis(),
    // Test event subscription
    testEvent: () => {
      const testListener = (bands) => {
        console.log("ðŸ§ª AudioProbe test listener:", bands);
      };
      AudioEngine.on('frame', testListener);
      console.log("âœ… Test listener registered. Check console for frame events.");
      return () => {
        AudioEngine.off('frame', testListener);
        console.log("ðŸ§¹ Test listener removed");
      };
    },
  };

  // Legacy alias
  window.AudioEngineProbe = window.AudioProbe;
}

// Optional auto-registration with HUD if present (non-breaking)
try {
  if (window?.hudCallbacks && typeof window.hudCallbacks === "object") {
    window.hudCallbacks.audio = () => AudioEngine.tick();
    LOG("ðŸ”— Registered hudCallbacks.audio");
  }
} catch {}

// ---- Legacy API compatibility (Phase 13.1) ----
// Preserve old function names for backward compatibility

export function initAudio() {
  LOG("initAudio() called (legacy API - no-op in Phase 13.1)");
}

export function enableAudio() {
  LOG("enableAudio() called (legacy API - delegating to AudioEngine.start())");
  console.log("ðŸ” Phase 13.1a: enableAudio() â†’ AudioEngine.start()");
  return AudioEngine.start();
}

export function updateAudio() {
  // Legacy no-op - audio updates are now passive via getSpectrum/getRMS
  AudioEngine.tick();
}

export function getAudioValues() {
  const rms = AudioEngine.getRMS();
  const spectrum = AudioEngine.getSpectrum();

  // Calculate legacy bass/mid/treble from spectrum
  const bass = spectrum.length > 0 ? spectrum.slice(0, 10).reduce((a, b) => a + b, 0) / 10 / 255 : 0;
  const mid = spectrum.length > 30 ? spectrum.slice(10, 30).reduce((a, b) => a + b, 0) / 20 / 255 : 0;
  const treble = spectrum.length > 50 ? spectrum.slice(30, 50).reduce((a, b) => a + b, 0) / 20 / 255 : 0;

  return {
    bass,
    mid,
    treble,
    level: rms,
    isEnabled: AudioEngine.state === "running"
  };
}

export function onAudioUpdate(callback) {
  // Legacy event-based API - convert to callback registration
  if (typeof callback === "function") {
    AudioEngine.onReady(callback);
  }
}

// Get audio context for recording (ensures context exists)
export function getAudioContext() {
  if (!AudioEngine.ctx) {
    const Ctx = window.AudioContext || window.webkitAudioContext;
    if (Ctx) {
      AudioEngine.ctx = new Ctx({ latencyHint: "interactive" });
      console.log("ðŸŽ¹ AudioContext created early for synth:", AudioEngine.ctx.state);
    }
  }
  return AudioEngine.ctx;
}
